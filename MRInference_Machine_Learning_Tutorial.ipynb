{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MRInference Machine Learning Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_S-nejYxXqL"
      },
      "source": [
        "# **Welcome to the MRInference machine learning tutorial!**\n",
        "by Sandra Vieira\n",
        "\n",
        "---\n",
        "This webpage contains a brief step-by-step tutorial on the implementation of a standard supervised machine learning pipeline using Python programming language. Before this tutorial make sure to go through the pre-recorded lectures:  \n",
        "\n",
        "*   Introduction to Machine Learning\n",
        "*   The Machine Learning Pipeline \n",
        "\n",
        "\n",
        "---\n",
        "##Machine Learning: Methods and Applications to Brain Disorders\n",
        "This tutorial and both pre-recorded lectures above are based on the book [Machine Learning: Methods and Applications to Brain Disorders](https://www.amazon.co.uk/Machine-Learning-Methods-Applications-Disorders/dp/0128157399). The pre-recorded lectures are based on chapters 1-3 and this tutorial is a shorter version of Chapter 19. You can access the full tutorial of Chapter 19 [here](https://github.com/MLMH-Lab/How-To-Build-A-Machine-Learning-Model).\n",
        "\n",
        "---  \n",
        "## Aim and structure of the tutorial\n",
        "For this tutorial you will use a toy dataset containing the grey matter volume and thickness from different brain regions extracted with FreeSurfer to classify patients with schizophrenia and healthy controls using a Support Vector Machine (SVM). The main steps of the tutrial will follow the pipeline presented in lecture The Machine Learning Pipeline and are shown in the figure below.\n",
        "\n",
        "![workflow](https://raw.githubusercontent.com/MLMH-Lab/How-To-Build-A-Machine-Learning-Model/master/figures/Figure%201.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DheC9LU-E_Ml"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1wCZCSk_uT1"
      },
      "source": [
        "Python language is organised in libraries. Each library contains a set of functions for a specific purpose. For example, numpy is a popular library for manipulating numerical data, while pandas is most commonly used to handle tabular data. There are several libraries for machine learning analysis; in this tutorial we will use scikitlearn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7eLJpXevBzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34692da6-701a-4cd9-b8fa-9164d2d949f6"
      },
      "source": [
        "# SNIPPET 1\n",
        "\n",
        "# Manipulate data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plots\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Statistical tests\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "\n",
        "# Ignore WARNING\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtZrMrJIHNwP"
      },
      "source": [
        "Some steps in our analysis will be subjected to randomness.  We should set the  seed value to a fixed number to guarantee that we get the same results every time we run the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bAhzMrvvSty"
      },
      "source": [
        "# SNIPPET 2\n",
        "random_seed = 1\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx0qP3KYFK5S"
      },
      "source": [
        "## Problem formulation\n",
        "\n",
        " In this tutorial, our machine learning problem is: \n",
        "\n",
        "> *Classify patients with schizophrenia and healthy controls using structural MRI data.*\n",
        "\n",
        "From this formulation we can derive the main elements of our machine learning problem:\n",
        "\n",
        "*   **Features**: Structural MRI data\n",
        "*   **Task**: Binary classification\n",
        "*   **Target**: Patients with schizophrenia and healthy controls\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpwLz1NFFV5t"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "The aim of this step is to perform a series of statistical analyses to get the data ready for the machine learning model. In this tutorial, we will assume the data is ready to be analysed. However, in a real project we would want to pay close attention to several things including class imbalance (N HC vs N SZ), missing data (data imputation?), confounding variables (age, sex?), dimensionality (N features vs N participants).\n",
        "\n",
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL9OuQCCvVlp"
      },
      "source": [
        "# SNIPPET 4\n",
        "# dataset_file = Path('./Chapter_19_data.csv')\n",
        "dataste_url = 'https://raw.githubusercontent.com/sandramv/MRInference_ML_Tutorial/main/ml_tutorial_data.csv'\n",
        "dataset_df = pd.read_csv(dataste_url, index_col='ID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQstxsXkvaSb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "7d9956de-ca14-434d-8cc2-2e8279953b33"
      },
      "source": [
        "# SNIPPET 6\n",
        "dataset_df[0:6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Diagnosis</th>\n",
              "      <th>Left Lateral Ventricle</th>\n",
              "      <th>Left Inf Lat Vent</th>\n",
              "      <th>Left Cerebellum White Matter</th>\n",
              "      <th>Left Cerebellum Cortex</th>\n",
              "      <th>Left Thalamus Proper</th>\n",
              "      <th>Left Caudate</th>\n",
              "      <th>Left Putamen</th>\n",
              "      <th>Left Pallidum</th>\n",
              "      <th>rd Ventricle</th>\n",
              "      <th>th Ventricle</th>\n",
              "      <th>Brain Stem</th>\n",
              "      <th>Left Hippocampus</th>\n",
              "      <th>Left Amygdala</th>\n",
              "      <th>CSF</th>\n",
              "      <th>Left Accumbens area</th>\n",
              "      <th>Left VentralDC</th>\n",
              "      <th>Right Lateral Ventricle</th>\n",
              "      <th>Right Inf Lat Vent</th>\n",
              "      <th>Right Cerebellum White Matter</th>\n",
              "      <th>Right Cerebellum Cortex</th>\n",
              "      <th>Right Thalamus Proper</th>\n",
              "      <th>Right Caudate</th>\n",
              "      <th>Right Putamen</th>\n",
              "      <th>Right Pallidum</th>\n",
              "      <th>Right Hippocampus</th>\n",
              "      <th>Right Amygdala</th>\n",
              "      <th>Right Accumbens area</th>\n",
              "      <th>Right VentralDC</th>\n",
              "      <th>CC Posterior</th>\n",
              "      <th>CC Mid Posterior</th>\n",
              "      <th>CC Central</th>\n",
              "      <th>CC Mid Anterior</th>\n",
              "      <th>CC Anterior</th>\n",
              "      <th>lh bankssts volume</th>\n",
              "      <th>lh caudalanteriorcingulate volume</th>\n",
              "      <th>lh caudalmiddlefrontal volume</th>\n",
              "      <th>lh cuneus volume</th>\n",
              "      <th>lh entorhinal volume</th>\n",
              "      <th>lh fusiform volume</th>\n",
              "      <th>...</th>\n",
              "      <th>lh superiortemporal thickness</th>\n",
              "      <th>lh supramarginal thickness</th>\n",
              "      <th>lh frontalpole thickness</th>\n",
              "      <th>lh temporalpole thickness</th>\n",
              "      <th>lh transversetemporal thickness</th>\n",
              "      <th>lh insula thickness</th>\n",
              "      <th>rh bankssts thickness</th>\n",
              "      <th>rh caudalanteriorcingulate thickness</th>\n",
              "      <th>rh caudalmiddlefrontal thickness</th>\n",
              "      <th>rh cuneus thickness</th>\n",
              "      <th>rh entorhinal thickness</th>\n",
              "      <th>rh fusiform thickness</th>\n",
              "      <th>rh inferiorparietal thickness</th>\n",
              "      <th>rh inferiortemporal thickness</th>\n",
              "      <th>rh isthmuscingulate thickness</th>\n",
              "      <th>rh lateraloccipital thickness</th>\n",
              "      <th>rh lateralorbitofrontal thickness</th>\n",
              "      <th>rh lingual thickness</th>\n",
              "      <th>rh medialorbitofrontal thickness</th>\n",
              "      <th>rh middletemporal thickness</th>\n",
              "      <th>rh parahippocampal thickness</th>\n",
              "      <th>rh paracentral thickness</th>\n",
              "      <th>rh parsopercularis thickness</th>\n",
              "      <th>rh parsorbitalis thickness</th>\n",
              "      <th>rh parstriangularis thickness</th>\n",
              "      <th>rh pericalcarine thickness</th>\n",
              "      <th>rh postcentral thickness</th>\n",
              "      <th>rh posteriorcingulate thickness</th>\n",
              "      <th>rh precentral thickness</th>\n",
              "      <th>rh precuneus thickness</th>\n",
              "      <th>rh rostralanteriorcingulate thickness</th>\n",
              "      <th>rh rostralmiddlefrontal thickness</th>\n",
              "      <th>rh superiorfrontal thickness</th>\n",
              "      <th>rh superiorparietal thickness</th>\n",
              "      <th>rh superiortemporal thickness</th>\n",
              "      <th>rh supramarginal thickness</th>\n",
              "      <th>rh frontalpole thickness</th>\n",
              "      <th>rh temporalpole thickness</th>\n",
              "      <th>rh transversetemporal thickness</th>\n",
              "      <th>rh insula thickness</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c001</th>\n",
              "      <td>hc</td>\n",
              "      <td>4226.907844</td>\n",
              "      <td>414.407845</td>\n",
              "      <td>12242.90784</td>\n",
              "      <td>43410.50784</td>\n",
              "      <td>7020.107844</td>\n",
              "      <td>4133.407844</td>\n",
              "      <td>6467.707844</td>\n",
              "      <td>2048.207844</td>\n",
              "      <td>825.507845</td>\n",
              "      <td>1751.707844</td>\n",
              "      <td>18918.50784</td>\n",
              "      <td>3423.907844</td>\n",
              "      <td>917.007845</td>\n",
              "      <td>907.207845</td>\n",
              "      <td>691.607845</td>\n",
              "      <td>3500.107844</td>\n",
              "      <td>3517.007844</td>\n",
              "      <td>568.407845</td>\n",
              "      <td>13079.90784</td>\n",
              "      <td>44261.50784</td>\n",
              "      <td>6855.307844</td>\n",
              "      <td>4248.407844</td>\n",
              "      <td>6746.707844</td>\n",
              "      <td>1941.307844</td>\n",
              "      <td>3427.807844</td>\n",
              "      <td>1297.107844</td>\n",
              "      <td>618.107845</td>\n",
              "      <td>3837.307844</td>\n",
              "      <td>812.507845</td>\n",
              "      <td>429.107845</td>\n",
              "      <td>520.707845</td>\n",
              "      <td>407.907845</td>\n",
              "      <td>745.407845</td>\n",
              "      <td>1242.007844</td>\n",
              "      <td>911.007845</td>\n",
              "      <td>4082.007844</td>\n",
              "      <td>3236.007844</td>\n",
              "      <td>1992.007844</td>\n",
              "      <td>8908.007844</td>\n",
              "      <td>...</td>\n",
              "      <td>2.464844</td>\n",
              "      <td>2.409844</td>\n",
              "      <td>2.712844</td>\n",
              "      <td>1.940844</td>\n",
              "      <td>2.206844</td>\n",
              "      <td>2.895844</td>\n",
              "      <td>2.320844</td>\n",
              "      <td>2.229844</td>\n",
              "      <td>2.517844</td>\n",
              "      <td>1.879844</td>\n",
              "      <td>3.004844</td>\n",
              "      <td>2.726844</td>\n",
              "      <td>2.516844</td>\n",
              "      <td>2.604844</td>\n",
              "      <td>2.430844</td>\n",
              "      <td>2.323844</td>\n",
              "      <td>2.507844</td>\n",
              "      <td>2.072844</td>\n",
              "      <td>2.571844</td>\n",
              "      <td>2.731844</td>\n",
              "      <td>2.866844</td>\n",
              "      <td>2.279844</td>\n",
              "      <td>2.505844</td>\n",
              "      <td>2.828844</td>\n",
              "      <td>2.433844</td>\n",
              "      <td>1.523844</td>\n",
              "      <td>1.999844</td>\n",
              "      <td>2.487844</td>\n",
              "      <td>2.484844</td>\n",
              "      <td>2.312844</td>\n",
              "      <td>2.440844</td>\n",
              "      <td>2.522844</td>\n",
              "      <td>2.656844</td>\n",
              "      <td>2.123844</td>\n",
              "      <td>2.638844</td>\n",
              "      <td>2.420844</td>\n",
              "      <td>2.489844</td>\n",
              "      <td>2.235844</td>\n",
              "      <td>2.300844</td>\n",
              "      <td>2.645844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c002</th>\n",
              "      <td>hc</td>\n",
              "      <td>4954.912699</td>\n",
              "      <td>414.812699</td>\n",
              "      <td>16519.51270</td>\n",
              "      <td>38808.31270</td>\n",
              "      <td>7013.312699</td>\n",
              "      <td>3882.912699</td>\n",
              "      <td>5781.012699</td>\n",
              "      <td>1735.912699</td>\n",
              "      <td>457.512699</td>\n",
              "      <td>1123.312699</td>\n",
              "      <td>20193.91270</td>\n",
              "      <td>3582.712699</td>\n",
              "      <td>1578.712699</td>\n",
              "      <td>708.212699</td>\n",
              "      <td>593.812699</td>\n",
              "      <td>3802.312699</td>\n",
              "      <td>3420.612699</td>\n",
              "      <td>258.512699</td>\n",
              "      <td>16028.81270</td>\n",
              "      <td>44035.41270</td>\n",
              "      <td>6654.512699</td>\n",
              "      <td>3477.012699</td>\n",
              "      <td>5121.112699</td>\n",
              "      <td>1619.612699</td>\n",
              "      <td>3322.212699</td>\n",
              "      <td>1402.112699</td>\n",
              "      <td>529.412699</td>\n",
              "      <td>3842.312699</td>\n",
              "      <td>1034.512699</td>\n",
              "      <td>446.112699</td>\n",
              "      <td>495.012699</td>\n",
              "      <td>772.512699</td>\n",
              "      <td>815.412699</td>\n",
              "      <td>2596.012699</td>\n",
              "      <td>1493.012699</td>\n",
              "      <td>7759.012699</td>\n",
              "      <td>3268.012699</td>\n",
              "      <td>1850.012699</td>\n",
              "      <td>9055.012699</td>\n",
              "      <td>...</td>\n",
              "      <td>2.635699</td>\n",
              "      <td>2.560699</td>\n",
              "      <td>3.013699</td>\n",
              "      <td>2.452699</td>\n",
              "      <td>2.308699</td>\n",
              "      <td>2.859699</td>\n",
              "      <td>2.573699</td>\n",
              "      <td>2.141699</td>\n",
              "      <td>2.687699</td>\n",
              "      <td>1.904699</td>\n",
              "      <td>3.248699</td>\n",
              "      <td>2.408699</td>\n",
              "      <td>2.485699</td>\n",
              "      <td>2.558699</td>\n",
              "      <td>2.233699</td>\n",
              "      <td>2.181699</td>\n",
              "      <td>2.595699</td>\n",
              "      <td>2.095699</td>\n",
              "      <td>2.375699</td>\n",
              "      <td>2.659699</td>\n",
              "      <td>2.301699</td>\n",
              "      <td>2.456699</td>\n",
              "      <td>2.426699</td>\n",
              "      <td>2.823699</td>\n",
              "      <td>2.513699</td>\n",
              "      <td>1.967699</td>\n",
              "      <td>2.002699</td>\n",
              "      <td>2.297699</td>\n",
              "      <td>2.625699</td>\n",
              "      <td>2.273699</td>\n",
              "      <td>2.507699</td>\n",
              "      <td>2.470699</td>\n",
              "      <td>2.645699</td>\n",
              "      <td>2.132699</td>\n",
              "      <td>2.848699</td>\n",
              "      <td>2.425699</td>\n",
              "      <td>2.883699</td>\n",
              "      <td>2.622699</td>\n",
              "      <td>2.322699</td>\n",
              "      <td>2.673699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c003</th>\n",
              "      <td>hc</td>\n",
              "      <td>4470.611989</td>\n",
              "      <td>370.111989</td>\n",
              "      <td>10193.51199</td>\n",
              "      <td>38637.51199</td>\n",
              "      <td>5802.911989</td>\n",
              "      <td>2941.711989</td>\n",
              "      <td>5802.511989</td>\n",
              "      <td>1467.411989</td>\n",
              "      <td>835.011989</td>\n",
              "      <td>1050.011989</td>\n",
              "      <td>17577.51199</td>\n",
              "      <td>3338.211989</td>\n",
              "      <td>1318.311989</td>\n",
              "      <td>754.911989</td>\n",
              "      <td>702.611989</td>\n",
              "      <td>3444.511989</td>\n",
              "      <td>4097.511989</td>\n",
              "      <td>157.611989</td>\n",
              "      <td>14706.71199</td>\n",
              "      <td>42082.01199</td>\n",
              "      <td>5799.311989</td>\n",
              "      <td>3225.411989</td>\n",
              "      <td>4863.311989</td>\n",
              "      <td>1402.311989</td>\n",
              "      <td>3645.711989</td>\n",
              "      <td>1347.911989</td>\n",
              "      <td>588.011989</td>\n",
              "      <td>3924.011989</td>\n",
              "      <td>1067.911989</td>\n",
              "      <td>450.011989</td>\n",
              "      <td>492.411989</td>\n",
              "      <td>476.011989</td>\n",
              "      <td>888.611989</td>\n",
              "      <td>2556.011989</td>\n",
              "      <td>1633.011989</td>\n",
              "      <td>6815.011989</td>\n",
              "      <td>3291.011989</td>\n",
              "      <td>1782.011989</td>\n",
              "      <td>10994.011990</td>\n",
              "      <td>...</td>\n",
              "      <td>2.410989</td>\n",
              "      <td>2.451989</td>\n",
              "      <td>2.125989</td>\n",
              "      <td>2.332989</td>\n",
              "      <td>2.057989</td>\n",
              "      <td>2.786989</td>\n",
              "      <td>2.409989</td>\n",
              "      <td>2.421989</td>\n",
              "      <td>2.680989</td>\n",
              "      <td>2.150989</td>\n",
              "      <td>3.018989</td>\n",
              "      <td>2.963989</td>\n",
              "      <td>2.510989</td>\n",
              "      <td>2.797989</td>\n",
              "      <td>2.518989</td>\n",
              "      <td>2.237989</td>\n",
              "      <td>2.512989</td>\n",
              "      <td>2.462989</td>\n",
              "      <td>2.787989</td>\n",
              "      <td>2.754989</td>\n",
              "      <td>2.674989</td>\n",
              "      <td>2.358989</td>\n",
              "      <td>2.418989</td>\n",
              "      <td>3.020989</td>\n",
              "      <td>2.570989</td>\n",
              "      <td>1.849989</td>\n",
              "      <td>2.188989</td>\n",
              "      <td>2.488989</td>\n",
              "      <td>2.679989</td>\n",
              "      <td>2.556989</td>\n",
              "      <td>2.545989</td>\n",
              "      <td>2.589989</td>\n",
              "      <td>2.885989</td>\n",
              "      <td>2.317989</td>\n",
              "      <td>2.326989</td>\n",
              "      <td>2.454989</td>\n",
              "      <td>2.482989</td>\n",
              "      <td>2.232989</td>\n",
              "      <td>2.267989</td>\n",
              "      <td>2.795989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c004</th>\n",
              "      <td>hc</td>\n",
              "      <td>7553.310654</td>\n",
              "      <td>521.010654</td>\n",
              "      <td>12716.01065</td>\n",
              "      <td>41933.31065</td>\n",
              "      <td>5998.310654</td>\n",
              "      <td>2869.110654</td>\n",
              "      <td>5854.810654</td>\n",
              "      <td>1886.210654</td>\n",
              "      <td>867.310654</td>\n",
              "      <td>1577.310654</td>\n",
              "      <td>17785.41065</td>\n",
              "      <td>3468.710654</td>\n",
              "      <td>1242.410654</td>\n",
              "      <td>858.910654</td>\n",
              "      <td>491.410654</td>\n",
              "      <td>3704.910654</td>\n",
              "      <td>4481.710654</td>\n",
              "      <td>392.610654</td>\n",
              "      <td>13933.11065</td>\n",
              "      <td>43434.61065</td>\n",
              "      <td>6052.810654</td>\n",
              "      <td>2965.610654</td>\n",
              "      <td>5342.810654</td>\n",
              "      <td>1882.910654</td>\n",
              "      <td>4024.410654</td>\n",
              "      <td>1469.510654</td>\n",
              "      <td>478.610654</td>\n",
              "      <td>3533.110654</td>\n",
              "      <td>793.110654</td>\n",
              "      <td>348.110654</td>\n",
              "      <td>406.910654</td>\n",
              "      <td>377.710654</td>\n",
              "      <td>793.910654</td>\n",
              "      <td>1959.010654</td>\n",
              "      <td>1299.010654</td>\n",
              "      <td>6208.010654</td>\n",
              "      <td>2800.010654</td>\n",
              "      <td>1567.010654</td>\n",
              "      <td>9986.010654</td>\n",
              "      <td>...</td>\n",
              "      <td>2.427654</td>\n",
              "      <td>2.436654</td>\n",
              "      <td>3.303654</td>\n",
              "      <td>2.576654</td>\n",
              "      <td>2.368654</td>\n",
              "      <td>2.714654</td>\n",
              "      <td>2.548654</td>\n",
              "      <td>2.257654</td>\n",
              "      <td>2.599654</td>\n",
              "      <td>1.736654</td>\n",
              "      <td>2.678654</td>\n",
              "      <td>2.410654</td>\n",
              "      <td>2.354654</td>\n",
              "      <td>2.489654</td>\n",
              "      <td>2.210654</td>\n",
              "      <td>2.185654</td>\n",
              "      <td>2.517654</td>\n",
              "      <td>2.098654</td>\n",
              "      <td>2.623654</td>\n",
              "      <td>2.749654</td>\n",
              "      <td>2.750654</td>\n",
              "      <td>2.458654</td>\n",
              "      <td>2.427654</td>\n",
              "      <td>2.654654</td>\n",
              "      <td>2.401654</td>\n",
              "      <td>1.660654</td>\n",
              "      <td>2.067654</td>\n",
              "      <td>2.420654</td>\n",
              "      <td>2.589654</td>\n",
              "      <td>2.327654</td>\n",
              "      <td>2.323654</td>\n",
              "      <td>2.411654</td>\n",
              "      <td>2.770654</td>\n",
              "      <td>2.149654</td>\n",
              "      <td>2.458654</td>\n",
              "      <td>2.307654</td>\n",
              "      <td>3.284654</td>\n",
              "      <td>1.956654</td>\n",
              "      <td>2.297654</td>\n",
              "      <td>2.731654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c005</th>\n",
              "      <td>hc</td>\n",
              "      <td>8785.212771</td>\n",
              "      <td>396.912771</td>\n",
              "      <td>12077.41277</td>\n",
              "      <td>41818.91277</td>\n",
              "      <td>5839.812771</td>\n",
              "      <td>3614.812771</td>\n",
              "      <td>6013.112771</td>\n",
              "      <td>1550.712771</td>\n",
              "      <td>1226.612771</td>\n",
              "      <td>1008.412771</td>\n",
              "      <td>19291.61277</td>\n",
              "      <td>2821.512771</td>\n",
              "      <td>1197.412771</td>\n",
              "      <td>770.612771</td>\n",
              "      <td>451.712771</td>\n",
              "      <td>3553.912771</td>\n",
              "      <td>5712.312771</td>\n",
              "      <td>416.112771</td>\n",
              "      <td>12102.51277</td>\n",
              "      <td>44240.81277</td>\n",
              "      <td>5555.912771</td>\n",
              "      <td>3736.412771</td>\n",
              "      <td>5476.512771</td>\n",
              "      <td>1682.112771</td>\n",
              "      <td>3220.512771</td>\n",
              "      <td>1477.212771</td>\n",
              "      <td>702.312771</td>\n",
              "      <td>4192.512771</td>\n",
              "      <td>787.912771</td>\n",
              "      <td>479.612771</td>\n",
              "      <td>454.812771</td>\n",
              "      <td>437.612771</td>\n",
              "      <td>619.112771</td>\n",
              "      <td>2154.012771</td>\n",
              "      <td>978.012771</td>\n",
              "      <td>6817.012771</td>\n",
              "      <td>2844.012771</td>\n",
              "      <td>1891.012771</td>\n",
              "      <td>8445.012771</td>\n",
              "      <td>...</td>\n",
              "      <td>2.626771</td>\n",
              "      <td>2.391771</td>\n",
              "      <td>3.504771</td>\n",
              "      <td>3.140771</td>\n",
              "      <td>2.145771</td>\n",
              "      <td>2.953771</td>\n",
              "      <td>2.398771</td>\n",
              "      <td>2.550771</td>\n",
              "      <td>2.465771</td>\n",
              "      <td>1.886771</td>\n",
              "      <td>3.068771</td>\n",
              "      <td>2.745771</td>\n",
              "      <td>2.466771</td>\n",
              "      <td>2.673771</td>\n",
              "      <td>2.310771</td>\n",
              "      <td>2.144771</td>\n",
              "      <td>2.888771</td>\n",
              "      <td>1.948771</td>\n",
              "      <td>2.621771</td>\n",
              "      <td>2.749771</td>\n",
              "      <td>2.876771</td>\n",
              "      <td>2.250771</td>\n",
              "      <td>2.570771</td>\n",
              "      <td>2.959771</td>\n",
              "      <td>2.487771</td>\n",
              "      <td>1.750771</td>\n",
              "      <td>1.882771</td>\n",
              "      <td>2.456771</td>\n",
              "      <td>2.399771</td>\n",
              "      <td>2.417771</td>\n",
              "      <td>3.211771</td>\n",
              "      <td>2.467771</td>\n",
              "      <td>2.772771</td>\n",
              "      <td>2.051771</td>\n",
              "      <td>2.588771</td>\n",
              "      <td>2.325771</td>\n",
              "      <td>3.266771</td>\n",
              "      <td>3.162771</td>\n",
              "      <td>2.081771</td>\n",
              "      <td>2.607771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c006</th>\n",
              "      <td>hc</td>\n",
              "      <td>5083.706643</td>\n",
              "      <td>172.106643</td>\n",
              "      <td>11927.50664</td>\n",
              "      <td>38730.80664</td>\n",
              "      <td>5693.506643</td>\n",
              "      <td>3422.606643</td>\n",
              "      <td>6301.706643</td>\n",
              "      <td>1466.806643</td>\n",
              "      <td>1103.306643</td>\n",
              "      <td>1927.206643</td>\n",
              "      <td>18726.90664</td>\n",
              "      <td>3207.606643</td>\n",
              "      <td>1350.906643</td>\n",
              "      <td>842.506643</td>\n",
              "      <td>696.606643</td>\n",
              "      <td>3449.906643</td>\n",
              "      <td>4331.606643</td>\n",
              "      <td>192.606643</td>\n",
              "      <td>10098.00664</td>\n",
              "      <td>35352.20664</td>\n",
              "      <td>5239.106643</td>\n",
              "      <td>3190.706643</td>\n",
              "      <td>5036.606643</td>\n",
              "      <td>1540.506643</td>\n",
              "      <td>2749.306643</td>\n",
              "      <td>1674.006643</td>\n",
              "      <td>574.306643</td>\n",
              "      <td>3910.706643</td>\n",
              "      <td>944.906643</td>\n",
              "      <td>435.406643</td>\n",
              "      <td>451.206643</td>\n",
              "      <td>500.906643</td>\n",
              "      <td>821.206643</td>\n",
              "      <td>2223.006643</td>\n",
              "      <td>1224.006643</td>\n",
              "      <td>5884.006643</td>\n",
              "      <td>3787.006643</td>\n",
              "      <td>1395.006643</td>\n",
              "      <td>9760.006643</td>\n",
              "      <td>...</td>\n",
              "      <td>2.633643</td>\n",
              "      <td>2.455643</td>\n",
              "      <td>2.539643</td>\n",
              "      <td>2.554643</td>\n",
              "      <td>2.377643</td>\n",
              "      <td>3.164643</td>\n",
              "      <td>2.445643</td>\n",
              "      <td>2.458643</td>\n",
              "      <td>2.526643</td>\n",
              "      <td>2.094643</td>\n",
              "      <td>3.689643</td>\n",
              "      <td>2.732643</td>\n",
              "      <td>2.440643</td>\n",
              "      <td>2.305643</td>\n",
              "      <td>2.256643</td>\n",
              "      <td>2.262643</td>\n",
              "      <td>2.703643</td>\n",
              "      <td>2.215643</td>\n",
              "      <td>2.749643</td>\n",
              "      <td>2.541643</td>\n",
              "      <td>2.932643</td>\n",
              "      <td>2.413643</td>\n",
              "      <td>2.505643</td>\n",
              "      <td>2.693643</td>\n",
              "      <td>2.456643</td>\n",
              "      <td>1.850643</td>\n",
              "      <td>2.172643</td>\n",
              "      <td>2.402643</td>\n",
              "      <td>2.633643</td>\n",
              "      <td>2.466643</td>\n",
              "      <td>2.562643</td>\n",
              "      <td>2.603643</td>\n",
              "      <td>2.948643</td>\n",
              "      <td>2.177643</td>\n",
              "      <td>2.489643</td>\n",
              "      <td>2.362643</td>\n",
              "      <td>2.314643</td>\n",
              "      <td>3.512643</td>\n",
              "      <td>2.591643</td>\n",
              "      <td>2.606643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6 rows Ã— 170 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Diagnosis  ...  rh insula thickness\n",
              "ID              ...                     \n",
              "c001        hc  ...             2.645844\n",
              "c002        hc  ...             2.673699\n",
              "c003        hc  ...             2.795989\n",
              "c004        hc  ...             2.731654\n",
              "c005        hc  ...             2.607771\n",
              "c006        hc  ...             2.606643\n",
              "\n",
              "[6 rows x 170 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYr7Ar56vzLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "592eae98-9cbc-42d4-daf4-7593de8e184d"
      },
      "source": [
        "# SNIPPET 8\n",
        "print('Number of features = %d' % dataset_df.shape[1])\n",
        "print('Number of participants = %d' % dataset_df.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of features = 170\n",
            "Number of participants = 740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkuKZn7Wv_ev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ec56bb-9160-4dfd-8f58-2d2e56104a8c"
      },
      "source": [
        "# SNIPPET 11\n",
        "dataset_df['Diagnosis'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sz    372\n",
              "hc    368\n",
              "Name: Diagnosis, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ged1gBNwF5NC"
      },
      "source": [
        "### Feature set and target\n",
        "\n",
        "Our next step is to retrieve the target and features from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE6VrdAIwVjS"
      },
      "source": [
        "# SNIPPET 17\n",
        "# Target\n",
        "targets_df = dataset_df['Diagnosis']\n",
        "\n",
        "# Features\n",
        "features_names = dataset_df.columns[1:]\n",
        "features_df = dataset_df[features_names]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MFnbQr7wWM6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "02d13112-7127-441d-9c79-22813bf1afe9"
      },
      "source": [
        "# SNIPPET 17a\n",
        "features_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Left Lateral Ventricle</th>\n",
              "      <th>Left Inf Lat Vent</th>\n",
              "      <th>Left Cerebellum White Matter</th>\n",
              "      <th>Left Cerebellum Cortex</th>\n",
              "      <th>Left Thalamus Proper</th>\n",
              "      <th>Left Caudate</th>\n",
              "      <th>Left Putamen</th>\n",
              "      <th>Left Pallidum</th>\n",
              "      <th>rd Ventricle</th>\n",
              "      <th>th Ventricle</th>\n",
              "      <th>Brain Stem</th>\n",
              "      <th>Left Hippocampus</th>\n",
              "      <th>Left Amygdala</th>\n",
              "      <th>CSF</th>\n",
              "      <th>Left Accumbens area</th>\n",
              "      <th>Left VentralDC</th>\n",
              "      <th>Right Lateral Ventricle</th>\n",
              "      <th>Right Inf Lat Vent</th>\n",
              "      <th>Right Cerebellum White Matter</th>\n",
              "      <th>Right Cerebellum Cortex</th>\n",
              "      <th>Right Thalamus Proper</th>\n",
              "      <th>Right Caudate</th>\n",
              "      <th>Right Putamen</th>\n",
              "      <th>Right Pallidum</th>\n",
              "      <th>Right Hippocampus</th>\n",
              "      <th>Right Amygdala</th>\n",
              "      <th>Right Accumbens area</th>\n",
              "      <th>Right VentralDC</th>\n",
              "      <th>CC Posterior</th>\n",
              "      <th>CC Mid Posterior</th>\n",
              "      <th>CC Central</th>\n",
              "      <th>CC Mid Anterior</th>\n",
              "      <th>CC Anterior</th>\n",
              "      <th>lh bankssts volume</th>\n",
              "      <th>lh caudalanteriorcingulate volume</th>\n",
              "      <th>lh caudalmiddlefrontal volume</th>\n",
              "      <th>lh cuneus volume</th>\n",
              "      <th>lh entorhinal volume</th>\n",
              "      <th>lh fusiform volume</th>\n",
              "      <th>lh inferiorparietal volume</th>\n",
              "      <th>...</th>\n",
              "      <th>lh superiortemporal thickness</th>\n",
              "      <th>lh supramarginal thickness</th>\n",
              "      <th>lh frontalpole thickness</th>\n",
              "      <th>lh temporalpole thickness</th>\n",
              "      <th>lh transversetemporal thickness</th>\n",
              "      <th>lh insula thickness</th>\n",
              "      <th>rh bankssts thickness</th>\n",
              "      <th>rh caudalanteriorcingulate thickness</th>\n",
              "      <th>rh caudalmiddlefrontal thickness</th>\n",
              "      <th>rh cuneus thickness</th>\n",
              "      <th>rh entorhinal thickness</th>\n",
              "      <th>rh fusiform thickness</th>\n",
              "      <th>rh inferiorparietal thickness</th>\n",
              "      <th>rh inferiortemporal thickness</th>\n",
              "      <th>rh isthmuscingulate thickness</th>\n",
              "      <th>rh lateraloccipital thickness</th>\n",
              "      <th>rh lateralorbitofrontal thickness</th>\n",
              "      <th>rh lingual thickness</th>\n",
              "      <th>rh medialorbitofrontal thickness</th>\n",
              "      <th>rh middletemporal thickness</th>\n",
              "      <th>rh parahippocampal thickness</th>\n",
              "      <th>rh paracentral thickness</th>\n",
              "      <th>rh parsopercularis thickness</th>\n",
              "      <th>rh parsorbitalis thickness</th>\n",
              "      <th>rh parstriangularis thickness</th>\n",
              "      <th>rh pericalcarine thickness</th>\n",
              "      <th>rh postcentral thickness</th>\n",
              "      <th>rh posteriorcingulate thickness</th>\n",
              "      <th>rh precentral thickness</th>\n",
              "      <th>rh precuneus thickness</th>\n",
              "      <th>rh rostralanteriorcingulate thickness</th>\n",
              "      <th>rh rostralmiddlefrontal thickness</th>\n",
              "      <th>rh superiorfrontal thickness</th>\n",
              "      <th>rh superiorparietal thickness</th>\n",
              "      <th>rh superiortemporal thickness</th>\n",
              "      <th>rh supramarginal thickness</th>\n",
              "      <th>rh frontalpole thickness</th>\n",
              "      <th>rh temporalpole thickness</th>\n",
              "      <th>rh transversetemporal thickness</th>\n",
              "      <th>rh insula thickness</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c001</th>\n",
              "      <td>4226.907844</td>\n",
              "      <td>414.407845</td>\n",
              "      <td>12242.907840</td>\n",
              "      <td>43410.50784</td>\n",
              "      <td>7020.107844</td>\n",
              "      <td>4133.407844</td>\n",
              "      <td>6467.707844</td>\n",
              "      <td>2048.207844</td>\n",
              "      <td>825.507845</td>\n",
              "      <td>1751.707844</td>\n",
              "      <td>18918.50784</td>\n",
              "      <td>3423.907844</td>\n",
              "      <td>917.007845</td>\n",
              "      <td>907.207845</td>\n",
              "      <td>691.607845</td>\n",
              "      <td>3500.107844</td>\n",
              "      <td>3517.007844</td>\n",
              "      <td>568.407845</td>\n",
              "      <td>13079.90784</td>\n",
              "      <td>44261.50784</td>\n",
              "      <td>6855.307844</td>\n",
              "      <td>4248.407844</td>\n",
              "      <td>6746.707844</td>\n",
              "      <td>1941.307844</td>\n",
              "      <td>3427.807844</td>\n",
              "      <td>1297.107844</td>\n",
              "      <td>618.107845</td>\n",
              "      <td>3837.307844</td>\n",
              "      <td>812.507845</td>\n",
              "      <td>429.107845</td>\n",
              "      <td>520.707845</td>\n",
              "      <td>407.907845</td>\n",
              "      <td>745.407845</td>\n",
              "      <td>1242.007844</td>\n",
              "      <td>911.007845</td>\n",
              "      <td>4082.007844</td>\n",
              "      <td>3236.007844</td>\n",
              "      <td>1992.007844</td>\n",
              "      <td>8908.007844</td>\n",
              "      <td>12371.007840</td>\n",
              "      <td>...</td>\n",
              "      <td>2.464844</td>\n",
              "      <td>2.409844</td>\n",
              "      <td>2.712844</td>\n",
              "      <td>1.940844</td>\n",
              "      <td>2.206844</td>\n",
              "      <td>2.895844</td>\n",
              "      <td>2.320844</td>\n",
              "      <td>2.229844</td>\n",
              "      <td>2.517844</td>\n",
              "      <td>1.879844</td>\n",
              "      <td>3.004844</td>\n",
              "      <td>2.726844</td>\n",
              "      <td>2.516844</td>\n",
              "      <td>2.604844</td>\n",
              "      <td>2.430844</td>\n",
              "      <td>2.323844</td>\n",
              "      <td>2.507844</td>\n",
              "      <td>2.072844</td>\n",
              "      <td>2.571844</td>\n",
              "      <td>2.731844</td>\n",
              "      <td>2.866844</td>\n",
              "      <td>2.279844</td>\n",
              "      <td>2.505844</td>\n",
              "      <td>2.828844</td>\n",
              "      <td>2.433844</td>\n",
              "      <td>1.523844</td>\n",
              "      <td>1.999844</td>\n",
              "      <td>2.487844</td>\n",
              "      <td>2.484844</td>\n",
              "      <td>2.312844</td>\n",
              "      <td>2.440844</td>\n",
              "      <td>2.522844</td>\n",
              "      <td>2.656844</td>\n",
              "      <td>2.123844</td>\n",
              "      <td>2.638844</td>\n",
              "      <td>2.420844</td>\n",
              "      <td>2.489844</td>\n",
              "      <td>2.235844</td>\n",
              "      <td>2.300844</td>\n",
              "      <td>2.645844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c002</th>\n",
              "      <td>4954.912699</td>\n",
              "      <td>414.812699</td>\n",
              "      <td>16519.512700</td>\n",
              "      <td>38808.31270</td>\n",
              "      <td>7013.312699</td>\n",
              "      <td>3882.912699</td>\n",
              "      <td>5781.012699</td>\n",
              "      <td>1735.912699</td>\n",
              "      <td>457.512699</td>\n",
              "      <td>1123.312699</td>\n",
              "      <td>20193.91270</td>\n",
              "      <td>3582.712699</td>\n",
              "      <td>1578.712699</td>\n",
              "      <td>708.212699</td>\n",
              "      <td>593.812699</td>\n",
              "      <td>3802.312699</td>\n",
              "      <td>3420.612699</td>\n",
              "      <td>258.512699</td>\n",
              "      <td>16028.81270</td>\n",
              "      <td>44035.41270</td>\n",
              "      <td>6654.512699</td>\n",
              "      <td>3477.012699</td>\n",
              "      <td>5121.112699</td>\n",
              "      <td>1619.612699</td>\n",
              "      <td>3322.212699</td>\n",
              "      <td>1402.112699</td>\n",
              "      <td>529.412699</td>\n",
              "      <td>3842.312699</td>\n",
              "      <td>1034.512699</td>\n",
              "      <td>446.112699</td>\n",
              "      <td>495.012699</td>\n",
              "      <td>772.512699</td>\n",
              "      <td>815.412699</td>\n",
              "      <td>2596.012699</td>\n",
              "      <td>1493.012699</td>\n",
              "      <td>7759.012699</td>\n",
              "      <td>3268.012699</td>\n",
              "      <td>1850.012699</td>\n",
              "      <td>9055.012699</td>\n",
              "      <td>12232.012700</td>\n",
              "      <td>...</td>\n",
              "      <td>2.635699</td>\n",
              "      <td>2.560699</td>\n",
              "      <td>3.013699</td>\n",
              "      <td>2.452699</td>\n",
              "      <td>2.308699</td>\n",
              "      <td>2.859699</td>\n",
              "      <td>2.573699</td>\n",
              "      <td>2.141699</td>\n",
              "      <td>2.687699</td>\n",
              "      <td>1.904699</td>\n",
              "      <td>3.248699</td>\n",
              "      <td>2.408699</td>\n",
              "      <td>2.485699</td>\n",
              "      <td>2.558699</td>\n",
              "      <td>2.233699</td>\n",
              "      <td>2.181699</td>\n",
              "      <td>2.595699</td>\n",
              "      <td>2.095699</td>\n",
              "      <td>2.375699</td>\n",
              "      <td>2.659699</td>\n",
              "      <td>2.301699</td>\n",
              "      <td>2.456699</td>\n",
              "      <td>2.426699</td>\n",
              "      <td>2.823699</td>\n",
              "      <td>2.513699</td>\n",
              "      <td>1.967699</td>\n",
              "      <td>2.002699</td>\n",
              "      <td>2.297699</td>\n",
              "      <td>2.625699</td>\n",
              "      <td>2.273699</td>\n",
              "      <td>2.507699</td>\n",
              "      <td>2.470699</td>\n",
              "      <td>2.645699</td>\n",
              "      <td>2.132699</td>\n",
              "      <td>2.848699</td>\n",
              "      <td>2.425699</td>\n",
              "      <td>2.883699</td>\n",
              "      <td>2.622699</td>\n",
              "      <td>2.322699</td>\n",
              "      <td>2.673699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c003</th>\n",
              "      <td>4470.611989</td>\n",
              "      <td>370.111989</td>\n",
              "      <td>10193.511990</td>\n",
              "      <td>38637.51199</td>\n",
              "      <td>5802.911989</td>\n",
              "      <td>2941.711989</td>\n",
              "      <td>5802.511989</td>\n",
              "      <td>1467.411989</td>\n",
              "      <td>835.011989</td>\n",
              "      <td>1050.011989</td>\n",
              "      <td>17577.51199</td>\n",
              "      <td>3338.211989</td>\n",
              "      <td>1318.311989</td>\n",
              "      <td>754.911989</td>\n",
              "      <td>702.611989</td>\n",
              "      <td>3444.511989</td>\n",
              "      <td>4097.511989</td>\n",
              "      <td>157.611989</td>\n",
              "      <td>14706.71199</td>\n",
              "      <td>42082.01199</td>\n",
              "      <td>5799.311989</td>\n",
              "      <td>3225.411989</td>\n",
              "      <td>4863.311989</td>\n",
              "      <td>1402.311989</td>\n",
              "      <td>3645.711989</td>\n",
              "      <td>1347.911989</td>\n",
              "      <td>588.011989</td>\n",
              "      <td>3924.011989</td>\n",
              "      <td>1067.911989</td>\n",
              "      <td>450.011989</td>\n",
              "      <td>492.411989</td>\n",
              "      <td>476.011989</td>\n",
              "      <td>888.611989</td>\n",
              "      <td>2556.011989</td>\n",
              "      <td>1633.011989</td>\n",
              "      <td>6815.011989</td>\n",
              "      <td>3291.011989</td>\n",
              "      <td>1782.011989</td>\n",
              "      <td>10994.011990</td>\n",
              "      <td>13008.011990</td>\n",
              "      <td>...</td>\n",
              "      <td>2.410989</td>\n",
              "      <td>2.451989</td>\n",
              "      <td>2.125989</td>\n",
              "      <td>2.332989</td>\n",
              "      <td>2.057989</td>\n",
              "      <td>2.786989</td>\n",
              "      <td>2.409989</td>\n",
              "      <td>2.421989</td>\n",
              "      <td>2.680989</td>\n",
              "      <td>2.150989</td>\n",
              "      <td>3.018989</td>\n",
              "      <td>2.963989</td>\n",
              "      <td>2.510989</td>\n",
              "      <td>2.797989</td>\n",
              "      <td>2.518989</td>\n",
              "      <td>2.237989</td>\n",
              "      <td>2.512989</td>\n",
              "      <td>2.462989</td>\n",
              "      <td>2.787989</td>\n",
              "      <td>2.754989</td>\n",
              "      <td>2.674989</td>\n",
              "      <td>2.358989</td>\n",
              "      <td>2.418989</td>\n",
              "      <td>3.020989</td>\n",
              "      <td>2.570989</td>\n",
              "      <td>1.849989</td>\n",
              "      <td>2.188989</td>\n",
              "      <td>2.488989</td>\n",
              "      <td>2.679989</td>\n",
              "      <td>2.556989</td>\n",
              "      <td>2.545989</td>\n",
              "      <td>2.589989</td>\n",
              "      <td>2.885989</td>\n",
              "      <td>2.317989</td>\n",
              "      <td>2.326989</td>\n",
              "      <td>2.454989</td>\n",
              "      <td>2.482989</td>\n",
              "      <td>2.232989</td>\n",
              "      <td>2.267989</td>\n",
              "      <td>2.795989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c004</th>\n",
              "      <td>7553.310654</td>\n",
              "      <td>521.010654</td>\n",
              "      <td>12716.010650</td>\n",
              "      <td>41933.31065</td>\n",
              "      <td>5998.310654</td>\n",
              "      <td>2869.110654</td>\n",
              "      <td>5854.810654</td>\n",
              "      <td>1886.210654</td>\n",
              "      <td>867.310654</td>\n",
              "      <td>1577.310654</td>\n",
              "      <td>17785.41065</td>\n",
              "      <td>3468.710654</td>\n",
              "      <td>1242.410654</td>\n",
              "      <td>858.910654</td>\n",
              "      <td>491.410654</td>\n",
              "      <td>3704.910654</td>\n",
              "      <td>4481.710654</td>\n",
              "      <td>392.610654</td>\n",
              "      <td>13933.11065</td>\n",
              "      <td>43434.61065</td>\n",
              "      <td>6052.810654</td>\n",
              "      <td>2965.610654</td>\n",
              "      <td>5342.810654</td>\n",
              "      <td>1882.910654</td>\n",
              "      <td>4024.410654</td>\n",
              "      <td>1469.510654</td>\n",
              "      <td>478.610654</td>\n",
              "      <td>3533.110654</td>\n",
              "      <td>793.110654</td>\n",
              "      <td>348.110654</td>\n",
              "      <td>406.910654</td>\n",
              "      <td>377.710654</td>\n",
              "      <td>793.910654</td>\n",
              "      <td>1959.010654</td>\n",
              "      <td>1299.010654</td>\n",
              "      <td>6208.010654</td>\n",
              "      <td>2800.010654</td>\n",
              "      <td>1567.010654</td>\n",
              "      <td>9986.010654</td>\n",
              "      <td>12236.010650</td>\n",
              "      <td>...</td>\n",
              "      <td>2.427654</td>\n",
              "      <td>2.436654</td>\n",
              "      <td>3.303654</td>\n",
              "      <td>2.576654</td>\n",
              "      <td>2.368654</td>\n",
              "      <td>2.714654</td>\n",
              "      <td>2.548654</td>\n",
              "      <td>2.257654</td>\n",
              "      <td>2.599654</td>\n",
              "      <td>1.736654</td>\n",
              "      <td>2.678654</td>\n",
              "      <td>2.410654</td>\n",
              "      <td>2.354654</td>\n",
              "      <td>2.489654</td>\n",
              "      <td>2.210654</td>\n",
              "      <td>2.185654</td>\n",
              "      <td>2.517654</td>\n",
              "      <td>2.098654</td>\n",
              "      <td>2.623654</td>\n",
              "      <td>2.749654</td>\n",
              "      <td>2.750654</td>\n",
              "      <td>2.458654</td>\n",
              "      <td>2.427654</td>\n",
              "      <td>2.654654</td>\n",
              "      <td>2.401654</td>\n",
              "      <td>1.660654</td>\n",
              "      <td>2.067654</td>\n",
              "      <td>2.420654</td>\n",
              "      <td>2.589654</td>\n",
              "      <td>2.327654</td>\n",
              "      <td>2.323654</td>\n",
              "      <td>2.411654</td>\n",
              "      <td>2.770654</td>\n",
              "      <td>2.149654</td>\n",
              "      <td>2.458654</td>\n",
              "      <td>2.307654</td>\n",
              "      <td>3.284654</td>\n",
              "      <td>1.956654</td>\n",
              "      <td>2.297654</td>\n",
              "      <td>2.731654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c005</th>\n",
              "      <td>8785.212771</td>\n",
              "      <td>396.912771</td>\n",
              "      <td>12077.412770</td>\n",
              "      <td>41818.91277</td>\n",
              "      <td>5839.812771</td>\n",
              "      <td>3614.812771</td>\n",
              "      <td>6013.112771</td>\n",
              "      <td>1550.712771</td>\n",
              "      <td>1226.612771</td>\n",
              "      <td>1008.412771</td>\n",
              "      <td>19291.61277</td>\n",
              "      <td>2821.512771</td>\n",
              "      <td>1197.412771</td>\n",
              "      <td>770.612771</td>\n",
              "      <td>451.712771</td>\n",
              "      <td>3553.912771</td>\n",
              "      <td>5712.312771</td>\n",
              "      <td>416.112771</td>\n",
              "      <td>12102.51277</td>\n",
              "      <td>44240.81277</td>\n",
              "      <td>5555.912771</td>\n",
              "      <td>3736.412771</td>\n",
              "      <td>5476.512771</td>\n",
              "      <td>1682.112771</td>\n",
              "      <td>3220.512771</td>\n",
              "      <td>1477.212771</td>\n",
              "      <td>702.312771</td>\n",
              "      <td>4192.512771</td>\n",
              "      <td>787.912771</td>\n",
              "      <td>479.612771</td>\n",
              "      <td>454.812771</td>\n",
              "      <td>437.612771</td>\n",
              "      <td>619.112771</td>\n",
              "      <td>2154.012771</td>\n",
              "      <td>978.012771</td>\n",
              "      <td>6817.012771</td>\n",
              "      <td>2844.012771</td>\n",
              "      <td>1891.012771</td>\n",
              "      <td>8445.012771</td>\n",
              "      <td>11016.012770</td>\n",
              "      <td>...</td>\n",
              "      <td>2.626771</td>\n",
              "      <td>2.391771</td>\n",
              "      <td>3.504771</td>\n",
              "      <td>3.140771</td>\n",
              "      <td>2.145771</td>\n",
              "      <td>2.953771</td>\n",
              "      <td>2.398771</td>\n",
              "      <td>2.550771</td>\n",
              "      <td>2.465771</td>\n",
              "      <td>1.886771</td>\n",
              "      <td>3.068771</td>\n",
              "      <td>2.745771</td>\n",
              "      <td>2.466771</td>\n",
              "      <td>2.673771</td>\n",
              "      <td>2.310771</td>\n",
              "      <td>2.144771</td>\n",
              "      <td>2.888771</td>\n",
              "      <td>1.948771</td>\n",
              "      <td>2.621771</td>\n",
              "      <td>2.749771</td>\n",
              "      <td>2.876771</td>\n",
              "      <td>2.250771</td>\n",
              "      <td>2.570771</td>\n",
              "      <td>2.959771</td>\n",
              "      <td>2.487771</td>\n",
              "      <td>1.750771</td>\n",
              "      <td>1.882771</td>\n",
              "      <td>2.456771</td>\n",
              "      <td>2.399771</td>\n",
              "      <td>2.417771</td>\n",
              "      <td>3.211771</td>\n",
              "      <td>2.467771</td>\n",
              "      <td>2.772771</td>\n",
              "      <td>2.051771</td>\n",
              "      <td>2.588771</td>\n",
              "      <td>2.325771</td>\n",
              "      <td>3.266771</td>\n",
              "      <td>3.162771</td>\n",
              "      <td>2.081771</td>\n",
              "      <td>2.607771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>p368</th>\n",
              "      <td>8283.573193</td>\n",
              "      <td>209.713156</td>\n",
              "      <td>12776.713160</td>\n",
              "      <td>50836.41316</td>\n",
              "      <td>7949.513156</td>\n",
              "      <td>3358.213156</td>\n",
              "      <td>5381.313156</td>\n",
              "      <td>1532.913156</td>\n",
              "      <td>1496.636755</td>\n",
              "      <td>3102.413156</td>\n",
              "      <td>20893.31316</td>\n",
              "      <td>4287.257241</td>\n",
              "      <td>1402.913156</td>\n",
              "      <td>1051.113156</td>\n",
              "      <td>472.913156</td>\n",
              "      <td>4016.113156</td>\n",
              "      <td>8589.813156</td>\n",
              "      <td>174.813156</td>\n",
              "      <td>12935.91316</td>\n",
              "      <td>52015.11316</td>\n",
              "      <td>6695.913156</td>\n",
              "      <td>3288.513156</td>\n",
              "      <td>4899.413156</td>\n",
              "      <td>1378.313156</td>\n",
              "      <td>4139.113156</td>\n",
              "      <td>1458.900500</td>\n",
              "      <td>411.313156</td>\n",
              "      <td>3816.313156</td>\n",
              "      <td>829.713156</td>\n",
              "      <td>405.313156</td>\n",
              "      <td>402.013156</td>\n",
              "      <td>407.813156</td>\n",
              "      <td>768.113156</td>\n",
              "      <td>2555.013156</td>\n",
              "      <td>1789.013156</td>\n",
              "      <td>6239.013156</td>\n",
              "      <td>2386.013156</td>\n",
              "      <td>1701.013156</td>\n",
              "      <td>8641.013156</td>\n",
              "      <td>12615.013160</td>\n",
              "      <td>...</td>\n",
              "      <td>2.680156</td>\n",
              "      <td>2.389156</td>\n",
              "      <td>3.127156</td>\n",
              "      <td>3.846156</td>\n",
              "      <td>2.002156</td>\n",
              "      <td>2.837156</td>\n",
              "      <td>2.263156</td>\n",
              "      <td>2.682156</td>\n",
              "      <td>2.447156</td>\n",
              "      <td>1.595156</td>\n",
              "      <td>3.442156</td>\n",
              "      <td>2.315156</td>\n",
              "      <td>2.152156</td>\n",
              "      <td>2.341156</td>\n",
              "      <td>2.223156</td>\n",
              "      <td>1.864156</td>\n",
              "      <td>2.377156</td>\n",
              "      <td>1.766156</td>\n",
              "      <td>2.317156</td>\n",
              "      <td>2.835156</td>\n",
              "      <td>2.473156</td>\n",
              "      <td>2.199156</td>\n",
              "      <td>2.599156</td>\n",
              "      <td>2.716156</td>\n",
              "      <td>2.423156</td>\n",
              "      <td>1.368156</td>\n",
              "      <td>1.783156</td>\n",
              "      <td>2.364156</td>\n",
              "      <td>2.115156</td>\n",
              "      <td>2.251156</td>\n",
              "      <td>2.853156</td>\n",
              "      <td>2.302156</td>\n",
              "      <td>2.660156</td>\n",
              "      <td>2.000156</td>\n",
              "      <td>2.656156</td>\n",
              "      <td>2.326156</td>\n",
              "      <td>2.853156</td>\n",
              "      <td>3.634156</td>\n",
              "      <td>2.129156</td>\n",
              "      <td>2.913156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>p369</th>\n",
              "      <td>5507.374607</td>\n",
              "      <td>96.003605</td>\n",
              "      <td>17634.203600</td>\n",
              "      <td>55609.30360</td>\n",
              "      <td>8785.803605</td>\n",
              "      <td>3592.103605</td>\n",
              "      <td>5334.103605</td>\n",
              "      <td>1546.903605</td>\n",
              "      <td>637.897786</td>\n",
              "      <td>1460.303605</td>\n",
              "      <td>23739.50360</td>\n",
              "      <td>4427.290443</td>\n",
              "      <td>1284.103605</td>\n",
              "      <td>1010.403605</td>\n",
              "      <td>435.603605</td>\n",
              "      <td>4206.003605</td>\n",
              "      <td>6587.903605</td>\n",
              "      <td>36.503605</td>\n",
              "      <td>15485.60360</td>\n",
              "      <td>57829.70360</td>\n",
              "      <td>7950.803605</td>\n",
              "      <td>3655.803605</td>\n",
              "      <td>5027.903605</td>\n",
              "      <td>1490.803605</td>\n",
              "      <td>3958.003605</td>\n",
              "      <td>1582.118722</td>\n",
              "      <td>455.003605</td>\n",
              "      <td>4103.503605</td>\n",
              "      <td>1141.803605</td>\n",
              "      <td>493.703605</td>\n",
              "      <td>527.503605</td>\n",
              "      <td>481.503605</td>\n",
              "      <td>937.903605</td>\n",
              "      <td>2075.003605</td>\n",
              "      <td>1735.003605</td>\n",
              "      <td>7747.003605</td>\n",
              "      <td>2789.003605</td>\n",
              "      <td>2718.003605</td>\n",
              "      <td>10033.003600</td>\n",
              "      <td>14443.003600</td>\n",
              "      <td>...</td>\n",
              "      <td>2.920605</td>\n",
              "      <td>2.617605</td>\n",
              "      <td>2.392605</td>\n",
              "      <td>3.746605</td>\n",
              "      <td>2.323605</td>\n",
              "      <td>3.132605</td>\n",
              "      <td>2.616605</td>\n",
              "      <td>2.521605</td>\n",
              "      <td>2.540605</td>\n",
              "      <td>1.793605</td>\n",
              "      <td>3.674605</td>\n",
              "      <td>2.709605</td>\n",
              "      <td>2.365605</td>\n",
              "      <td>2.636605</td>\n",
              "      <td>2.615605</td>\n",
              "      <td>2.203605</td>\n",
              "      <td>2.443605</td>\n",
              "      <td>2.025605</td>\n",
              "      <td>2.395605</td>\n",
              "      <td>2.825605</td>\n",
              "      <td>2.630605</td>\n",
              "      <td>2.147605</td>\n",
              "      <td>2.556605</td>\n",
              "      <td>2.181605</td>\n",
              "      <td>2.548605</td>\n",
              "      <td>1.433605</td>\n",
              "      <td>2.026605</td>\n",
              "      <td>2.653605</td>\n",
              "      <td>2.258605</td>\n",
              "      <td>2.414605</td>\n",
              "      <td>2.813605</td>\n",
              "      <td>2.487605</td>\n",
              "      <td>2.645605</td>\n",
              "      <td>2.216605</td>\n",
              "      <td>2.887605</td>\n",
              "      <td>2.476605</td>\n",
              "      <td>2.811605</td>\n",
              "      <td>3.901605</td>\n",
              "      <td>2.093605</td>\n",
              "      <td>2.892605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>p370</th>\n",
              "      <td>3607.623866</td>\n",
              "      <td>305.201604</td>\n",
              "      <td>13822.401600</td>\n",
              "      <td>52828.50160</td>\n",
              "      <td>8195.201604</td>\n",
              "      <td>3126.401604</td>\n",
              "      <td>5213.201604</td>\n",
              "      <td>1604.101604</td>\n",
              "      <td>905.963115</td>\n",
              "      <td>1382.001604</td>\n",
              "      <td>20869.10160</td>\n",
              "      <td>4570.248506</td>\n",
              "      <td>1689.301604</td>\n",
              "      <td>782.001604</td>\n",
              "      <td>441.401604</td>\n",
              "      <td>3746.901604</td>\n",
              "      <td>4239.601604</td>\n",
              "      <td>225.801604</td>\n",
              "      <td>14815.60160</td>\n",
              "      <td>55345.60160</td>\n",
              "      <td>6983.401604</td>\n",
              "      <td>3062.401604</td>\n",
              "      <td>5005.201604</td>\n",
              "      <td>1517.001604</td>\n",
              "      <td>4494.101604</td>\n",
              "      <td>1527.116406</td>\n",
              "      <td>389.501604</td>\n",
              "      <td>3588.801604</td>\n",
              "      <td>757.401604</td>\n",
              "      <td>470.101604</td>\n",
              "      <td>359.001604</td>\n",
              "      <td>411.101604</td>\n",
              "      <td>610.301604</td>\n",
              "      <td>2301.001604</td>\n",
              "      <td>1253.001604</td>\n",
              "      <td>5886.001604</td>\n",
              "      <td>2618.001604</td>\n",
              "      <td>2218.001604</td>\n",
              "      <td>8632.001604</td>\n",
              "      <td>9469.001604</td>\n",
              "      <td>...</td>\n",
              "      <td>2.784604</td>\n",
              "      <td>2.519604</td>\n",
              "      <td>2.336604</td>\n",
              "      <td>3.972604</td>\n",
              "      <td>2.306604</td>\n",
              "      <td>3.058604</td>\n",
              "      <td>2.764604</td>\n",
              "      <td>2.758604</td>\n",
              "      <td>2.575604</td>\n",
              "      <td>1.598604</td>\n",
              "      <td>3.495604</td>\n",
              "      <td>2.744604</td>\n",
              "      <td>2.581604</td>\n",
              "      <td>2.503604</td>\n",
              "      <td>2.554604</td>\n",
              "      <td>2.002604</td>\n",
              "      <td>2.597604</td>\n",
              "      <td>1.795604</td>\n",
              "      <td>2.701604</td>\n",
              "      <td>3.024604</td>\n",
              "      <td>2.879604</td>\n",
              "      <td>2.326604</td>\n",
              "      <td>2.547604</td>\n",
              "      <td>2.789604</td>\n",
              "      <td>2.493604</td>\n",
              "      <td>1.483604</td>\n",
              "      <td>2.042604</td>\n",
              "      <td>2.872604</td>\n",
              "      <td>2.382604</td>\n",
              "      <td>2.486604</td>\n",
              "      <td>3.044604</td>\n",
              "      <td>2.538604</td>\n",
              "      <td>2.888604</td>\n",
              "      <td>2.301604</td>\n",
              "      <td>2.892604</td>\n",
              "      <td>2.644604</td>\n",
              "      <td>2.761604</td>\n",
              "      <td>4.059604</td>\n",
              "      <td>2.571604</td>\n",
              "      <td>3.066604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>p371</th>\n",
              "      <td>8276.575805</td>\n",
              "      <td>255.310420</td>\n",
              "      <td>11482.110420</td>\n",
              "      <td>56545.41042</td>\n",
              "      <td>7251.510420</td>\n",
              "      <td>3445.010420</td>\n",
              "      <td>5015.910420</td>\n",
              "      <td>1368.610420</td>\n",
              "      <td>1382.500931</td>\n",
              "      <td>1586.810420</td>\n",
              "      <td>20569.51042</td>\n",
              "      <td>3291.992360</td>\n",
              "      <td>1424.510420</td>\n",
              "      <td>1080.510420</td>\n",
              "      <td>356.410420</td>\n",
              "      <td>3511.310420</td>\n",
              "      <td>7552.710420</td>\n",
              "      <td>154.810420</td>\n",
              "      <td>12207.41042</td>\n",
              "      <td>57303.01042</td>\n",
              "      <td>6515.710420</td>\n",
              "      <td>3310.910420</td>\n",
              "      <td>4492.610420</td>\n",
              "      <td>1399.410420</td>\n",
              "      <td>4070.510420</td>\n",
              "      <td>1415.569344</td>\n",
              "      <td>346.510420</td>\n",
              "      <td>3330.710420</td>\n",
              "      <td>708.310420</td>\n",
              "      <td>316.010420</td>\n",
              "      <td>323.510420</td>\n",
              "      <td>320.110420</td>\n",
              "      <td>638.610420</td>\n",
              "      <td>2373.010420</td>\n",
              "      <td>1369.010420</td>\n",
              "      <td>8252.010420</td>\n",
              "      <td>2423.010420</td>\n",
              "      <td>2188.010420</td>\n",
              "      <td>10296.010420</td>\n",
              "      <td>11911.010420</td>\n",
              "      <td>...</td>\n",
              "      <td>3.104420</td>\n",
              "      <td>2.800420</td>\n",
              "      <td>2.762420</td>\n",
              "      <td>4.052420</td>\n",
              "      <td>2.630420</td>\n",
              "      <td>3.166420</td>\n",
              "      <td>2.916420</td>\n",
              "      <td>2.663420</td>\n",
              "      <td>2.755420</td>\n",
              "      <td>1.733420</td>\n",
              "      <td>3.558420</td>\n",
              "      <td>2.621420</td>\n",
              "      <td>2.676420</td>\n",
              "      <td>2.420420</td>\n",
              "      <td>2.598420</td>\n",
              "      <td>2.329420</td>\n",
              "      <td>2.145420</td>\n",
              "      <td>2.047420</td>\n",
              "      <td>2.110420</td>\n",
              "      <td>3.341420</td>\n",
              "      <td>2.863420</td>\n",
              "      <td>2.567420</td>\n",
              "      <td>2.715420</td>\n",
              "      <td>2.504420</td>\n",
              "      <td>2.482420</td>\n",
              "      <td>1.524420</td>\n",
              "      <td>2.022420</td>\n",
              "      <td>2.756420</td>\n",
              "      <td>2.481420</td>\n",
              "      <td>2.589420</td>\n",
              "      <td>3.165420</td>\n",
              "      <td>2.564420</td>\n",
              "      <td>3.058420</td>\n",
              "      <td>2.413420</td>\n",
              "      <td>3.037420</td>\n",
              "      <td>2.916420</td>\n",
              "      <td>3.010420</td>\n",
              "      <td>4.361420</td>\n",
              "      <td>2.700420</td>\n",
              "      <td>2.631420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>p372</th>\n",
              "      <td>5170.559424</td>\n",
              "      <td>29.916186</td>\n",
              "      <td>9641.516186</td>\n",
              "      <td>43018.21619</td>\n",
              "      <td>6520.416186</td>\n",
              "      <td>3059.316186</td>\n",
              "      <td>3620.616186</td>\n",
              "      <td>918.416186</td>\n",
              "      <td>856.983376</td>\n",
              "      <td>1478.116186</td>\n",
              "      <td>14495.41619</td>\n",
              "      <td>3388.976822</td>\n",
              "      <td>1095.616186</td>\n",
              "      <td>866.816186</td>\n",
              "      <td>255.316186</td>\n",
              "      <td>2760.016186</td>\n",
              "      <td>4441.016186</td>\n",
              "      <td>159.516186</td>\n",
              "      <td>10677.21619</td>\n",
              "      <td>43584.81619</td>\n",
              "      <td>5360.416186</td>\n",
              "      <td>2842.916186</td>\n",
              "      <td>3613.516186</td>\n",
              "      <td>1147.316186</td>\n",
              "      <td>3562.316186</td>\n",
              "      <td>961.179582</td>\n",
              "      <td>258.516186</td>\n",
              "      <td>2981.716186</td>\n",
              "      <td>797.816186</td>\n",
              "      <td>389.316186</td>\n",
              "      <td>416.816186</td>\n",
              "      <td>427.316186</td>\n",
              "      <td>813.616186</td>\n",
              "      <td>2560.016186</td>\n",
              "      <td>1337.016186</td>\n",
              "      <td>5819.016186</td>\n",
              "      <td>2432.016186</td>\n",
              "      <td>1450.016186</td>\n",
              "      <td>9843.016186</td>\n",
              "      <td>10247.016190</td>\n",
              "      <td>...</td>\n",
              "      <td>2.908186</td>\n",
              "      <td>2.740186</td>\n",
              "      <td>2.841186</td>\n",
              "      <td>4.247186</td>\n",
              "      <td>2.427186</td>\n",
              "      <td>3.432186</td>\n",
              "      <td>2.755186</td>\n",
              "      <td>3.213186</td>\n",
              "      <td>2.758186</td>\n",
              "      <td>1.828186</td>\n",
              "      <td>2.586186</td>\n",
              "      <td>2.925186</td>\n",
              "      <td>2.638186</td>\n",
              "      <td>2.482186</td>\n",
              "      <td>2.792186</td>\n",
              "      <td>2.263186</td>\n",
              "      <td>2.667186</td>\n",
              "      <td>2.080186</td>\n",
              "      <td>2.250186</td>\n",
              "      <td>3.178186</td>\n",
              "      <td>2.912186</td>\n",
              "      <td>2.690186</td>\n",
              "      <td>2.908186</td>\n",
              "      <td>2.560186</td>\n",
              "      <td>2.964186</td>\n",
              "      <td>1.486186</td>\n",
              "      <td>2.322186</td>\n",
              "      <td>2.924186</td>\n",
              "      <td>2.643186</td>\n",
              "      <td>2.441186</td>\n",
              "      <td>3.650186</td>\n",
              "      <td>2.586186</td>\n",
              "      <td>3.084186</td>\n",
              "      <td>2.346186</td>\n",
              "      <td>2.986186</td>\n",
              "      <td>2.778186</td>\n",
              "      <td>2.939186</td>\n",
              "      <td>3.889186</td>\n",
              "      <td>2.191186</td>\n",
              "      <td>3.330186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>740 rows Ã— 169 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Left Lateral Ventricle  ...  rh insula thickness\n",
              "ID                            ...                     \n",
              "c001             4226.907844  ...             2.645844\n",
              "c002             4954.912699  ...             2.673699\n",
              "c003             4470.611989  ...             2.795989\n",
              "c004             7553.310654  ...             2.731654\n",
              "c005             8785.212771  ...             2.607771\n",
              "...                      ...  ...                  ...\n",
              "p368             8283.573193  ...             2.913156\n",
              "p369             5507.374607  ...             2.892605\n",
              "p370             3607.623866  ...             3.066604\n",
              "p371             8276.575805  ...             2.631420\n",
              "p372             5170.559424  ...             3.330186\n",
              "\n",
              "[740 rows x 169 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFWhHg90wa3q"
      },
      "source": [
        "# SNIPPET 19\n",
        "targets_df = targets_df.map({'hc': 0, 'sz': 1})\n",
        "targets = targets_df.values.astype('int')\n",
        "\n",
        "features = features_df.values.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgznrUlMGB1G"
      },
      "source": [
        "## Feature engineering\n",
        "\n",
        "\n",
        "### Feature extraction\n",
        "In our example, we want to use neuroanatomical data to classify SZ and HC. This requires the extraction of brain morphometric information from the raw MRI images.\n",
        "\n",
        "\n",
        "### Feature scaling/normalization \n",
        "In brain disorders research, we often deal with datasets that contain features that vary in units and range. However, to model the data correctly and effectively, most machine learning algorithms require the data to be on the same scale. Since normalization involves statistics (e.g. mean and variance) of the set used to train the model, in this point we split the data into training and test sets following the scheme a cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdHzxs9Owblf"
      },
      "source": [
        "# SNIPPET 20\n",
        "n_folds = 10\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5Dqk9KEgfhZ"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/MLMH-Lab/How-To-Build-A-Machine-Learning-Model/master/figures/Figure%202.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrowgunAwehw"
      },
      "source": [
        "# SNIPPET 21\n",
        "predictions_df = pd.DataFrame(targets_df)\n",
        "predictions_df['predictions'] = np.nan\n",
        "\n",
        "bac_cv = np.zeros((n_folds, 1))\n",
        "sens_cv = np.zeros((n_folds, 1))\n",
        "spec_cv = np.zeros((n_folds, 1))\n",
        "coef_cv = np.zeros((n_folds, len(features_names)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0VLOeOuwkNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "546958db-c4ae-4a37-fca2-df798aeeadfa"
      },
      "source": [
        "# SNIPPET 22 REDO WITHOUT CV?!\n",
        "for i_fold, (train_idx, test_idx) in enumerate(skf.split(features, targets)):\n",
        "    features_train, features_test = features[train_idx], features[test_idx]\n",
        "    targets_train, targets_test = targets[train_idx], targets[test_idx]\n",
        "\n",
        "    print('CV iteration: %d' % (i_fold + 1))\n",
        "    print('Training set size: %d' % len(targets_train))\n",
        "    print('Test set size: %d' % len(targets_test))\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 23\n",
        "    # Feature scaling/normalization\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    scaler.fit(features_train)\n",
        "\n",
        "    features_train_norm = scaler.transform(features_train)\n",
        "    features_test_norm = scaler.transform(features_test)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 24\n",
        "    # Here, we will use the linear kernel, as this will make it easier to extract the coefficients\n",
        "    #  of the SVM model (feature importance) later on.\n",
        "    clf = LinearSVC(loss='hinge')\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 25\n",
        "    # SVM relies on a hyperparameter C that regulates how much we want to avoid misclassifying each\n",
        "    #  training example.\n",
        "\n",
        "    # Hyper-parameter search space\n",
        "    param_grid = {'C': [2 ** -6, 2 ** -5, 2 ** -4, 2 ** -3, 2 ** -2, 2 ** -1, 2 ** 0, 2 ** 1]}\n",
        "\n",
        "    # Grid search\n",
        "    internal_cv = StratifiedKFold(n_splits=10)\n",
        "    grid_cv = GridSearchCV(estimator=clf,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=internal_cv,\n",
        "                           scoring='balanced_accuracy',\n",
        "                           verbose=1)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 26\n",
        "    # Model training\n",
        "    grid_result = grid_cv.fit(features_train_norm, targets_train)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 27\n",
        "    print('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_))\n",
        "    means = grid_result.cv_results_['mean_test_score']\n",
        "    stds = grid_result.cv_results_['std_test_score']\n",
        "    params = grid_result.cv_results_['params']\n",
        "\n",
        "    for mean, stdev, param in zip(means, stds, params):\n",
        "        print('%f (%f) with: %r' % (mean, stdev, param))\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 28\n",
        "    best_clf = grid_cv.best_estimator_\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 30\n",
        "    # Model evaluation\n",
        "    # Finally, we use the final trained model best_clf to make predictions in the test set.\n",
        "    target_test_predicted = best_clf.predict(features_test_norm)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 31\n",
        "    print('Confusion matrix')\n",
        "    cm = confusion_matrix(targets_test, target_test_predicted)\n",
        "    print(cm)\n",
        "\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    bac_test = balanced_accuracy_score(targets_test, target_test_predicted)\n",
        "    sens_test = tp / (tp + fn)\n",
        "    spec_test = tn / (tn + fp)\n",
        "\n",
        "    print('Balanced accuracy: %.3f ' % bac_test)\n",
        "    print('Sensitivity: %.3f ' % sens_test)\n",
        "    print('Specificity: %.3f ' % spec_test)\n",
        "\n",
        "    bac_cv[i_fold, :] = bac_test\n",
        "    sens_cv[i_fold, :] = sens_test\n",
        "    spec_cv[i_fold, :] = spec_test\n",
        "    print('--------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV iteration: 1\n",
            "Training set size: 666\n",
            "Test set size: 74\n",
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.681996 using {'C': 0.03125}\n",
            "0.656239 (0.082053) with: {'C': 0.015625}\n",
            "0.681996 (0.084610) with: {'C': 0.03125}\n",
            "0.672950 (0.086047) with: {'C': 0.0625}\n",
            "0.681863 (0.084353) with: {'C': 0.125}\n",
            "0.660561 (0.069703) with: {'C': 0.25}\n",
            "0.663592 (0.070552) with: {'C': 0.5}\n",
            "0.660517 (0.063467) with: {'C': 1}\n",
            "0.650000 (0.070569) with: {'C': 2}\n",
            "Confusion matrix\n",
            "[[29  8]\n",
            " [13 24]]\n",
            "Balanced accuracy: 0.716 \n",
            "Sensitivity: 0.649 \n",
            "Specificity: 0.784 \n",
            "--------------------------------------------------------------------------\n",
            "CV iteration: 2\n",
            "Training set size: 666\n",
            "Test set size: 74\n",
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.684537 using {'C': 0.25}\n",
            "0.657442 (0.060636) with: {'C': 0.015625}\n",
            "0.675535 (0.071928) with: {'C': 0.03125}\n",
            "0.683200 (0.069288) with: {'C': 0.0625}\n",
            "0.683066 (0.061052) with: {'C': 0.125}\n",
            "0.684537 (0.068300) with: {'C': 0.25}\n",
            "0.678565 (0.069170) with: {'C': 0.5}\n",
            "0.655971 (0.067399) with: {'C': 1}\n",
            "0.650089 (0.080730) with: {'C': 2}\n",
            "Confusion matrix\n",
            "[[27 10]\n",
            " [10 27]]\n",
            "Balanced accuracy: 0.730 \n",
            "Sensitivity: 0.730 \n",
            "Specificity: 0.730 \n",
            "--------------------------------------------------------------------------\n",
            "CV iteration: 3\n",
            "Training set size: 666\n",
            "Test set size: 74\n",
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.678565 using {'C': 0.25}\n",
            "0.671257 (0.083172) with: {'C': 0.015625}\n",
            "0.657843 (0.090354) with: {'C': 0.03125}\n",
            "0.672504 (0.071297) with: {'C': 0.0625}\n",
            "0.675579 (0.085828) with: {'C': 0.125}\n",
            "0.678565 (0.096041) with: {'C': 0.25}\n",
            "0.675535 (0.100533) with: {'C': 0.5}\n",
            "0.656105 (0.095358) with: {'C': 1}\n",
            "0.656283 (0.092216) with: {'C': 2}\n",
            "Confusion matrix\n",
            "[[26 11]\n",
            " [12 25]]\n",
            "Balanced accuracy: 0.689 \n",
            "Sensitivity: 0.676 \n",
            "Specificity: 0.703 \n",
            "--------------------------------------------------------------------------\n",
            "CV iteration: 4\n",
            "Training set size: 666\n",
            "Test set size: 74\n",
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.677050 using {'C': 0.125}\n",
            "0.666667 (0.087044) with: {'C': 0.015625}\n",
            "0.659091 (0.086701) with: {'C': 0.03125}\n",
            "0.672549 (0.076349) with: {'C': 0.0625}\n",
            "0.677050 (0.074486) with: {'C': 0.125}\n",
            "0.665152 (0.066903) with: {'C': 0.25}\n",
            "0.648574 (0.062914) with: {'C': 0.5}\n",
            "0.657665 (0.074417) with: {'C': 1}\n",
            "0.663681 (0.085317) with: {'C': 2}\n",
            "Confusion matrix\n",
            "[[23 14]\n",
            " [ 6 31]]\n",
            "Balanced accuracy: 0.730 \n",
            "Sensitivity: 0.838 \n",
            "Specificity: 0.622 \n",
            "--------------------------------------------------------------------------\n",
            "CV iteration: 5\n",
            "Training set size: 666\n",
            "Test set size: 74\n",
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.660829 using {'C': 0.125}\n",
            "0.654768 (0.066926) with: {'C': 0.015625}\n",
            "0.645766 (0.077823) with: {'C': 0.03125}\n",
            "0.639840 (0.074877) with: {'C': 0.0625}\n",
            "0.660829 (0.072983) with: {'C': 0.125}\n",
            "0.651738 (0.076762) with: {'C': 0.25}\n",
            "0.635294 (0.085356) with: {'C': 0.5}\n",
            "0.632308 (0.097250) with: {'C': 1}\n",
            "0.636809 (0.080568) with: {'C': 2}\n",
            "Confusion matrix\n",
            "[[22 15]\n",
            " [ 3 34]]\n",
            "Balanced accuracy: 0.757 \n",
            "Sensitivity: 0.919 \n",
            "Specificity: 0.595 \n",
            "--------------------------------------------------------------------------\n",
            "CV iteration: 6\n",
            "Training set size: 666\n",
            "Test set size: 74\n",
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.651337 using {'C': 0.125}\n",
            "0.615553 (0.087564) with: {'C': 0.015625}\n",
            "0.622995 (0.085502) with: {'C': 0.03125}\n",
            "0.634804 (0.080206) with: {'C': 0.0625}\n",
            "0.651337 (0.075556) with: {'C': 0.125}\n",
            "0.633422 (0.063303) with: {'C': 0.25}\n",
            "0.623039 (0.082717) with: {'C': 0.5}\n",
            "0.630704 (0.073362) with: {'C': 1}\n",
            "0.644430 (0.074283) with: {'C': 2}\n",
            "Confusion matrix\n",
            "[[31  6]\n",
            " [ 8 29]]\n",
            "Balanced accuracy: 0.811 \n",
            "Sensitivity: 0.784 \n",
            "Specificity: 0.838 \n",
            "--------------------------------------------------------------------------\n",
            "CV iteration: 7\n",
            "Training set size: 666\n",
            "Test set size: 74\n",
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.665152 using {'C': 0.125}\n",
            "0.662166 (0.086544) with: {'C': 0.015625}\n",
            "0.659046 (0.087600) with: {'C': 0.03125}\n",
            "0.657487 (0.074425) with: {'C': 0.0625}\n",
            "0.665152 (0.077787) with: {'C': 0.125}\n",
            "0.653075 (0.056146) with: {'C': 0.25}\n",
            "0.634848 (0.078374) with: {'C': 0.5}\n",
            "0.622772 (0.076761) with: {'C': 1}\n",
            "0.630348 (0.072909) with: {'C': 2}\n",
            "Confusion matrix\n",
            "[[29  8]\n",
            " [11 26]]\n",
            "Balanced accuracy: 0.743 \n",
            "Sensitivity: 0.703 \n",
            "Specificity: 0.784 \n",
            "--------------------------------------------------------------------------\n",
            "CV iteration: 8\n",
            "Training set size: 666\n",
            "Test set size: 74\n",
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.672906 using {'C': 0.03125}\n",
            "0.659492 (0.097342) with: {'C': 0.015625}\n",
            "0.672906 (0.087826) with: {'C': 0.03125}\n",
            "0.662389 (0.078545) with: {'C': 0.0625}\n",
            "0.666934 (0.089875) with: {'C': 0.125}\n",
            "0.657888 (0.086627) with: {'C': 0.25}\n",
            "0.653298 (0.080025) with: {'C': 0.5}\n",
            "0.654813 (0.077727) with: {'C': 1}\n",
            "0.645900 (0.069074) with: {'C': 2}\n",
            "Confusion matrix\n",
            "[[30  7]\n",
            " [10 27]]\n",
            "Balanced accuracy: 0.770 \n",
            "Sensitivity: 0.730 \n",
            "Specificity: 0.811 \n",
            "--------------------------------------------------------------------------\n",
            "CV iteration: 9\n",
            "Training set size: 666\n",
            "Test set size: 74\n",
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.672193 using {'C': 0.0625}\n",
            "0.648128 (0.076796) with: {'C': 0.015625}\n",
            "0.652674 (0.078949) with: {'C': 0.03125}\n",
            "0.672193 (0.068385) with: {'C': 0.0625}\n",
            "0.651471 (0.077171) with: {'C': 0.125}\n",
            "0.658913 (0.077373) with: {'C': 0.25}\n",
            "0.640686 (0.073699) with: {'C': 0.5}\n",
            "0.640686 (0.085128) with: {'C': 1}\n",
            "0.642291 (0.081810) with: {'C': 2}\n",
            "Confusion matrix\n",
            "[[26 10]\n",
            " [ 9 29]]\n",
            "Balanced accuracy: 0.743 \n",
            "Sensitivity: 0.763 \n",
            "Specificity: 0.722 \n",
            "--------------------------------------------------------------------------\n",
            "CV iteration: 10\n",
            "Training set size: 666\n",
            "Test set size: 74\n",
            "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
            "Best: 0.681684 using {'C': 0.125}\n",
            "0.661943 (0.088166) with: {'C': 0.015625}\n",
            "0.664929 (0.102367) with: {'C': 0.03125}\n",
            "0.676961 (0.082928) with: {'C': 0.0625}\n",
            "0.681684 (0.070080) with: {'C': 0.125}\n",
            "0.678788 (0.071619) with: {'C': 0.25}\n",
            "0.660695 (0.059294) with: {'C': 0.5}\n",
            "0.653119 (0.067356) with: {'C': 1}\n",
            "0.657754 (0.081060) with: {'C': 2}\n",
            "Confusion matrix\n",
            "[[28  8]\n",
            " [16 22]]\n",
            "Balanced accuracy: 0.678 \n",
            "Sensitivity: 0.579 \n",
            "Specificity: 0.778 \n",
            "--------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    7.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7XK73lWwlYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27504c5-e019-4171-e1a9-b144bb6e250c"
      },
      "source": [
        "# SNIPPET 32\n",
        "print('CV results')\n",
        "print('Bac: Mean(SD) = %.3f(%.3f)' % (bac_cv.mean(), bac_cv.std()))\n",
        "print('Sens: Mean(SD) = %.3f(%.3f)' % (sens_cv.mean(), sens_cv.std()))\n",
        "print('Spec: Mean(SD) = %.3f(%.3f)' % (spec_cv.mean(), spec_cv.std()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV results\n",
            "Bac: Mean(SD) = 0.737(0.036)\n",
            "Sens: Mean(SD) = 0.737(0.092)\n",
            "Spec: Mean(SD) = 0.736(0.075)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QjfcX04Gurp"
      },
      "source": [
        "## Post-hoc analysis\n",
        "\n",
        "Once we have our final model, we can run several additional analyses. This tutotial does not include these analysis, but we could look at the following:\n",
        "\n",
        "*   Test balanced accuracy, sensitivity and specificity for statistical significance via permutation testing\n",
        "*   Identify the features that provided the greatest contribution to the task "
      ]
    }
  ]
}