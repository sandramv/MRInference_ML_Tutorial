{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MRInference Machine Learning Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_S-nejYxXqL"
      },
      "source": [
        "# **Welcome to the MRInference machine learning tutorial!**\n",
        "by Sandra Vieira\n",
        "\n",
        "---\n",
        "This webpage contains a brief step-by-step tutorial on the implementation of a standard supervised machine learning pipeline using Python programming language. Before this tutorial make sure to go through the pre-recorded lectures:  \n",
        "\n",
        "*   Introduction to Machine Learning\n",
        "*   The Machine Learning Pipeline\n",
        "\n",
        "---\n",
        "##Machine Learning: Methods and Applications to Brain Disorders\n",
        "This tutorial and both pre-recorded lectures above are based on the book [Machine Learning: Methods and Applications to Brain Disorders](https://www.amazon.co.uk/Machine-Learning-Methods-Applications-Disorders/dp/0128157399). The pre-recorded lectures are based on chapters 1-3 and this tutorial is a shorter version of Chapter 19. You can access the full tutorial of Chapter 19 [here](https://github.com/MLMH-Lab/How-To-Build-A-Machine-Learning-Model).\n",
        "\n",
        "---  \n",
        "## Aim and structure of the tutorial\n",
        "For this tutorial you will use a toy dataset containing the grey matter volume and thickness from different brain regions extracted with FreeSurfer to classify patients with schizophrenia and healthy controls using a Support Vector Machine (SVM). The script and data for the tutorial are stored [here](https://github.com/sandramv/MRInference_ML_Tutorial). The main steps of the tutrial will follow the pipeline presented in the lecture The Machine Learning Pipeline and are shown in the figure below.\n",
        "\n",
        "![workflow](https://raw.githubusercontent.com/sandramv/MRInference_ML_Tutorial/master/Figures/pipeline.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DheC9LU-E_Ml"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1wCZCSk_uT1"
      },
      "source": [
        "Python language is organised in libraries. Each library contains a set of functions for a specific purpose. For example, numpy is a popular library for manipulating numerical data, while pandas is most commonly used to handle tabular data. There are several libraries for machine learning analysis; in this tutorial we will use scikitlearn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7eLJpXevBzb"
      },
      "source": [
        "# SNIPPET 1: import libraries\n",
        "\n",
        "# Manipulate data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plots\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Statistical tests\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "# Ignore WARNING\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtZrMrJIHNwP"
      },
      "source": [
        "## Set random seed\n",
        "Some steps in our analysis will be subjected to randomness.  We should set the  seed value to a fixed number to guarantee that we get the same results every time we run the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bAhzMrvvSty"
      },
      "source": [
        "# SNIPPET 2: set random seed\n",
        "random_seed = 1\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx0qP3KYFK5S"
      },
      "source": [
        "## 1. Problem formulation\n",
        "\n",
        " In this tutorial, our machine learning problem is: \n",
        "\n",
        "> *Classify patients with schizophrenia and healthy controls using structural MRI data.*\n",
        "\n",
        "From this formulation we can derive the main elements of our machine learning problem:\n",
        "\n",
        "*   **Features**: Structural MRI data\n",
        "*   **Task**: Binary classification\n",
        "*   **Target**: Patients with schizophrenia and healthy controls\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpwLz1NFFV5t"
      },
      "source": [
        "## 2. Data preparation\n",
        "\n",
        "The aim of this step is to perform a series of statistical analyses to get the data ready for the machine learning model. In this tutorial, we will assume the data is ready to be analysed. However, in a real project we would want to pay close attention to several things including class imbalance (N HC vs N SZ), missing data (data imputation?), confounding variables (age, sex?), dimensionality (N features vs N participants)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ3-vvh6GT_x"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL9OuQCCvVlp"
      },
      "source": [
        "# SNIPPET 3: load data\n",
        "dataste_url = 'https://raw.githubusercontent.com/sandramv/MRInference_ML_Tutorial/main/Data/ml_tutorial_data.csv'\n",
        "dataset_df = pd.read_csv(dataste_url, index_col='ID')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQstxsXkvaSb"
      },
      "source": [
        "# SNIPPET 4: preview data\n",
        "dataset_df[0:6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYr7Ar56vzLQ"
      },
      "source": [
        "# SNIPPET 5: sample size and number of features\n",
        "print('Number of features = %d' % dataset_df.shape[1])\n",
        "print('Number of participants = %d' % dataset_df.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkuKZn7Wv_ev"
      },
      "source": [
        "# SNIPPET 6: number of healthy controls (HC=0) and patients (SZ=1)\n",
        "dataset_df['Diagnosis'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ged1gBNwF5NC"
      },
      "source": [
        "### Feature set and target\n",
        "\n",
        "Our next step is to retrieve the target and features from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE6VrdAIwVjS"
      },
      "source": [
        "# SNIPPET 7: get target and input features\n",
        "targets_df = dataset_df['Diagnosis']\n",
        "\n",
        "features_names = dataset_df.columns[3:]\n",
        "features_df = dataset_df[features_names]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MFnbQr7wWM6"
      },
      "source": [
        "# SNIPPET 8: see features\n",
        "features_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgznrUlMGB1G"
      },
      "source": [
        "## 3. Feature engineering\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy9S4cTuHCuo"
      },
      "source": [
        "### Feature extraction\n",
        "In our example, we want to use neuroanatomical data to classify SZ and HC. This requires the extraction of brain morphometric information from the raw MRI images. This step has already been done, i.e. the csv file already contains these data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXTGICjTA3C5"
      },
      "source": [
        "## 4. Model training  \n",
        "The first thing we need to do at this step is to setup the **cross-validation (CV) scheme**. Then we iterate over each CV fold and train and test the model at each one. The snippets below:  \n",
        "* Setup de CV scheme \n",
        "* Change data type of features and target variables   \n",
        "* Create structure to hold the results from each CV fold  \n",
        "* Iterate over each cv fold (snippet 22) and: \n",
        "  * Split all data into train and test sets  \n",
        "  * Normalize data\n",
        "  * Define machine learning algorithm\n",
        "  * Fit algorithm to the train set\n",
        "  * Make predictions in the test set\n",
        "  * Compute performance metrics in the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdHzxs9Owblf"
      },
      "source": [
        "# SNIPPET 9: setup cross-validation (cv) scheme\n",
        "n_folds = 10\n",
        "skf = StratifiedKFold(n_splits=n_folds, random_state=random_seed)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5Dqk9KEgfhZ"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/sandramv/MRInference_ML_Tutorial/master/Figures/crossvalidation.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZhlTPYFkTvX"
      },
      "source": [
        "# SNIPPET 10: change data type \n",
        "targets = targets_df.values.astype('int')\n",
        "features = features_df.values.astype('float32')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrowgunAwehw"
      },
      "source": [
        "# SNIPPET 11: create structure to hold the results from each cross-validaiton fold\n",
        "acc_cv = np.zeros((n_folds, 1))\n",
        "bac_cv = np.zeros((n_folds, 1))\n",
        "sens_cv = np.zeros((n_folds, 1))\n",
        "spec_cv = np.zeros((n_folds, 1))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0VLOeOuwkNk"
      },
      "source": [
        "# SNIPPET 12: iterate over each cv fold\n",
        "for i_fold, (train_idx, test_idx) in enumerate(skf.split(features, targets)):\n",
        "\n",
        "    # SNIPPET 13: split data into train and test sets\n",
        "    features_train, features_test = features[train_idx], features[test_idx]\n",
        "    targets_train, targets_test = targets[train_idx], targets[test_idx]\n",
        "\n",
        "    print('CV iteration: %d' % (i_fold + 1))\n",
        "    print('Training set size: %d' % len(targets_train))\n",
        "    print('Test set size: %d' % len(targets_test))\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 14: normalize data\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    scaler.fit(features_train)\n",
        "\n",
        "    features_train_norm = scaler.transform(features_train)\n",
        "    features_test_norm = scaler.transform(features_test)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 15: define and train the classifier (SVM)\n",
        "    clf = LinearSVC(loss='hinge')\n",
        "    clf.fit(features_train_norm, targets_train)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 16: make predictions in the test set\n",
        "    target_test_predicted = clf.predict(features_test_norm)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # SNIPPET 17: compute performance metrics in the test set\n",
        "    print('Confusion matrix')\n",
        "    cm = confusion_matrix(targets_test, target_test_predicted)\n",
        "    print(cm)\n",
        "\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    acc_test = accuracy_score(targets_test, target_test_predicted)\n",
        "    bac_test = balanced_accuracy_score(targets_test, target_test_predicted)\n",
        "    sens_test = tp / (tp + fn)\n",
        "    spec_test = tn / (tn + fp)\n",
        "\n",
        "    print('Accuracy: %.3f ' % acc_test)\n",
        "    print('Balanced accuracy: %.3f ' % bac_test)\n",
        "    print('Sensitivity: %.3f ' % sens_test)\n",
        "    print('Specificity: %.3f ' % spec_test)\n",
        "\n",
        "    acc_cv[i_fold, :] = acc_test\n",
        "    bac_cv[i_fold, :] = bac_test\n",
        "    sens_cv[i_fold, :] = sens_test\n",
        "    spec_cv[i_fold, :] = spec_test\n",
        "    print('--------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbv8asp7FRHx"
      },
      "source": [
        "## 5. Model evaluation  \n",
        "Once the model training and testing in the CV scheme is finished, we compute the overall performace of the model by taking the mean performance across the CV folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7XK73lWwlYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dec028-9aff-46c7-d8b0-0fb06d300b09"
      },
      "source": [
        "# SNIPPET 18\n",
        "print('CV results')\n",
        "print('Acc: Mean(SD) = %.3f(%.3f)' % (acc_cv.mean(), acc_cv.std()))\n",
        "print('Bac: Mean(SD) = %.3f(%.3f)' % (bac_cv.mean(), bac_cv.std()))\n",
        "print('Sens: Mean(SD) = %.3f(%.3f)' % (sens_cv.mean(), sens_cv.std()))\n",
        "print('Spec: Mean(SD) = %.3f(%.3f)' % (spec_cv.mean(), spec_cv.std()))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV results\n",
            "Acc: Mean(SD) = 0.885(0.054)\n",
            "Bac: Mean(SD) = 0.827(0.083)\n",
            "Sens: Mean(SD) = 0.747(0.185)\n",
            "Spec: Mean(SD) = 0.908(0.071)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QjfcX04Gurp"
      },
      "source": [
        "## 6. Post-hoc analysis\n",
        "\n",
        "Once we have our final model, we can run several additional analyses. This tutotial does not include these analysis, but we could look at the following:\n",
        "\n",
        "*   Test balanced accuracy, sensitivity and specificity for statistical significance via permutation testing\n",
        "*   Identify the features that provided the greatest contribution to the task "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g1zm4O1WKyQ"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "### Exercise 1\n",
        "Review the pipeline above and answer the questions below.  \n",
        "1.1. What are the features, target variable and task? What does features, target variable and task mean?  \n",
        "1.2. Is this a classification or regression problem? Why?  \n",
        "1.3. What cross-validation scheme used? Explain in your own words how it works.  \n",
        "1.4. What machine learning algorithm was used?  \n",
        "1.5. How was the data normalized (hint: google the method used and click on the sklearn website)  \n",
        "1.6. Why should the data be normalized? Why was the train and test data normalized separately?  \n",
        "1.7. What is the value inside the parenthesis for each performance metric and what does it mean in this context?    \n",
        "1.8. What is Snippet 2 doing and why do we need it?  \n",
        "\n",
        "### Exercise 2\n",
        "2.1. What is the difference between accuracy and balanced accuracy? Are they different or the same in this exercise? If they are different, explain why this is the case.  \n",
        "2.2. What does 75% sensitivity mean?  \n",
        "2.3. Run the snippet below and explain what it shows. Would you have any concerns about the results above with this information? If you do, how would you address them?  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESO7j30X5eup"
      },
      "source": [
        "# SNIPPET 19\n",
        "sns.countplot(x='Diagnosis', hue='Sex', data=dataset_df, palette=['#839098', '#f7d842'])\n",
        "plt.legend(['Male', 'Female'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3qQQRL6CkXu"
      },
      "source": [
        "2.4. Can you think of any confounding variables that may be influencing the result?  "
      ]
    }
  ]
}